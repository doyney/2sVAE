{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pythae.models import VAE, VAEConfig, AutoModel\n",
    "from pythae.data.datasets import DatasetOutput\n",
    "from pythae.samplers import (\n",
    "    TwoStageVAESampler,\n",
    "    TwoStageVAESamplerConfig,\n",
    ")\n",
    "from pythae.trainers import BaseTrainerConfig\n",
    "\n",
    "import gradio as gr\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.transforms import ToPILImage\n",
    "import traceback\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "class UTKFaceDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        super().__init__(root=root, transform=transform, target_transform=target_transform)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, _ = self.imgs[index]\n",
    "        X, _ = super().__getitem__(index)\n",
    "        filename = os.path.basename(path)\n",
    "        return DatasetOutput(data=X, filename=filename)\n",
    "\n",
    "\n",
    "class DatasetOutput:\n",
    "    def __init__(self, data, filename):\n",
    "        self.data = data\n",
    "        self.filename = filename\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "all_dataset = UTKFaceDataset(root=\"./data\", transform=transform)\n",
    "\n",
    "trained_model = AutoModel.load_from_folder(\"./my_model/bestmodel/final_model\")\n",
    "\n",
    "twostagevae_config = TwoStageVAESamplerConfig(\n",
    "    reconstruction_loss='mse', \n",
    ")\n",
    "\n",
    "twostage_sampler = TwoStageVAESampler(\n",
    "    model=trained_model,\n",
    "    sampler_config=twostagevae_config\n",
    ")\n",
    "\n",
    "two_stage_train_config = BaseTrainerConfig(\n",
    "    output_dir='my_model',\n",
    "    learning_rate=0.00014,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_epochs=75,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-06T17:03:01.340952Z",
     "start_time": "2024-07-06T17:02:56.470629Z"
    }
   },
   "id": "f394f770b99c3f6"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def filter_dataset(race, gender, age_input): #taks ~1000 seconds with entire dataset (23,000 images)\n",
    "    try:\n",
    "        gender_map = {\"Any\": None, \"Male\": 0, \"Female\": 1}\n",
    "        race_map = {\"Any\": None, \"White\": 0, \"Black\": 1, \"Asian\": 2, \"Indian\": 3}\n",
    "    \n",
    "        gender_code = gender_map[gender]\n",
    "        race_code = race_map[race]\n",
    "    \n",
    "        if '-' in age_input: \n",
    "            age_range = [int(a) for a in age_input.split('-')]\n",
    "        elif age_input: \n",
    "            age_range = [int(age_input), int(age_input)]\n",
    "        else:\n",
    "            age_range = [0, 116]\n",
    "    \n",
    "        filtered_dataset = []\n",
    "        for item in all_dataset:\n",
    "            _, filename = item.data, item.filename\n",
    "            parts = filename.split('_')\n",
    "            \n",
    "            if len(parts) == 4:\n",
    "                file_age, file_gender, file_race, _ = parts\n",
    "                age_condition = age_range[0] <= int(file_age) <= age_range[1]\n",
    "                gender_condition = True if gender_code is None else int(file_gender) == gender_code\n",
    "                race_condition = True if race_code is None else int(file_race) == race_code\n",
    "    \n",
    "                if age_condition and gender_condition and race_condition:\n",
    "                    filtered_dataset.append(item)\n",
    "        \n",
    "        if filtered_dataset:\n",
    "            num_images_to_show = min(len(filtered_dataset), 25)\n",
    "            indices = torch.randperm(len(filtered_dataset))[:num_images_to_show]\n",
    "            images = [filtered_dataset[i].data for i in indices]\n",
    "            grid = vutils.make_grid(images, nrow=5)\n",
    "            attribute_data_pil = ToPILImage()(grid)\n",
    "    \n",
    "            filtered_dataset_tensors = [item.data for item in filtered_dataset]  \n",
    "            filtered_dataset_tensor = torch.stack(filtered_dataset_tensors)\n",
    "\n",
    "            split_index = int(0.8 * len(filtered_dataset_tensor))\n",
    "            filtered_dataset_train = filtered_dataset_tensor[:split_index]\n",
    "            filtered_dataset_eval = filtered_dataset_tensor[split_index:]\n",
    "    \n",
    "            twostage_sampler.fit(\n",
    "                train_data=filtered_dataset_train,\n",
    "                eval_data=filtered_dataset_eval,\n",
    "                training_config=two_stage_train_config,\n",
    "            )\n",
    "            \n",
    "            # Sample from the model\n",
    "            gen_data2 = twostage_sampler.sample(num_samples=25)\n",
    "            condition_grid = vutils.make_grid(gen_data2, nrow=5, normalize=True, scale_each=True)\n",
    "            conditioned_data_pil = ToPILImage()(condition_grid)\n",
    "            \n",
    "            return f\"Found {len(filtered_dataset)} images\", attribute_data_pil, conditioned_data_pil\n",
    "        else:\n",
    "            return \"No images found\", None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        error_traceback = traceback.format_exc()\n",
    "        full_error_message = f\"An error occurred: {error_message}\\n\\nTraceback:\\n{error_traceback}\"\n",
    "        print(full_error_message) \n",
    "        return full_error_message, None, None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-06T17:03:01.349137Z",
     "start_time": "2024-07-06T17:03:01.345077Z"
    }
   },
   "id": "e459358ac75be60e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genders = [\"Any\", \"Male\", \"Female\"]\n",
    "races = [\"Any\", \"White\", \"Black\", \"Asian\", \"Indian\"]\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=filter_dataset,\n",
    "    inputs=[\n",
    "        gr.Dropdown(choices=races, label=\"Race\"),\n",
    "        gr.Dropdown(choices=genders, label=\"Gender\"),\n",
    "        gr.Textbox(label=\"Age (specific, range like 20-30, or leave blank for all)\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        \"text\",\n",
    "        \"image\",\n",
    "        \"image\"\n",
    "    ]\n",
    ")\n",
    "iface.launch()\n",
    "#works better using local URL\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-06T17:03:01.475434Z",
     "start_time": "2024-07-06T17:03:01.348619Z"
    }
   },
   "id": "ac9a6d90b8fc98f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
